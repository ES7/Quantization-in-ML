{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6tCv-A_bJVb"
      },
      "source": [
        "# Linear Quantization\n",
        "Quantization is a process of mapping a large set to a small set of values. For example applying 8-bit linear quantization on the following matrix:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "191.6 & -13.5 & 728.6 \\\\\n",
        "92.14 & 295.5 & -184 \\\\\n",
        "0 & 684.6 & 245.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Quantized Matrix:**\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-23 & -81 & 127 \\\\\n",
        "-51 & 6 & -128 \\\\\n",
        "-77 & 114 & -8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "we can map the most positive number in the matrix **(728.6)** to the maximum value that **int8** can store, which is **127**. Similarly the most negative number **(-184)** to **-128**. Then by following a linear mapping we can map the rest of the values. After this we can delete the original tensor to free up the space and end up with the quantized tensors with parameters **s (scale)** and **z (zero point)** that we used to perform linear mapping.\n",
        "\n",
        "## How can we go the other way back to the original tensor?\n",
        "We can apply the same mapping but we won't get the same values. That means quantization results in loss of information. By applying the same linear qunatization on the quantized matrix we get the dequantized matirx.\n",
        "\n",
        "**Dequantized Matrix:**\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "193.2 & -14.3 & 730.1 \\\\\n",
        "93.1 & 297 & -182.5 \\\\\n",
        "0 & 683.6 & 246.9\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Error Matrix:**\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1.66 & 0.82 & 1.48 \\\\\n",
        "0.91 & 1.54 & 1.48 \\\\\n",
        "0 & 1.04 & 1.44\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can see that the values of the original matrix and dequantized matrix are approximately the same. The error matrix is the difference of original and dequnatized one and we can see the error is not zero but not too bad either."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "id": "ZT2yDY5gJlIr",
        "outputId": "5122c4cc-3b31-42d8-b116-a2c598788948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/sh: 1: pip: not found\n",
            "/usr/bin/sh: 1: pip: not found\n",
            "/usr/bin/sh: 1: pip: not found\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install quanto\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "SfljpOFIJlIs"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-small\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "height": 47,
        "id": "vx554czjJlIs"
      },
      "source": [
        "Flan-T5 is a variant of the T5 (Text-to-Text Transfer Transformer) model, which is a popular and powerful transformer-based model for natural language processing (NLP) tasks. Flan-T5 is specifically designed for few-shot learning, which means it can perform well even when trained on a small amount of labeled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "id": "L8Z2oobsJlIt",
        "outputId": "9798bda2-1e1d-4d57-ae46-e0c6a7f1d8a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer is used to transform the text into a list of tokens that the model is able to understand."
      ],
      "metadata": {
        "id": "HZtrTLmMJwKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "_9GMntZAJlIt"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "id": "zxeH9vG9JlIt",
        "outputId": "e81c56ab-ea41-4b00-9a24-1dcc0217529d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> annie scott</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Hello, my name is \"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "id": "KXHcpZQRJlIt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def named_module_tensors(module, recurse=False):\n",
        "    for named_parameter in module.named_parameters(recurse=recurse):\n",
        "      name, val = named_parameter\n",
        "      flag = True\n",
        "      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n",
        "        if hasattr(val,\"_data\"):\n",
        "          yield name + \"._data\", val._data\n",
        "        if hasattr(val,\"_scale\"):\n",
        "          yield name + \"._scale\", val._scale\n",
        "      else:\n",
        "        yield named_parameter\n",
        "\n",
        "    for named_buffer in module.named_buffers(recurse=recurse):\n",
        "      yield named_buffer\n",
        "\n",
        "def dtype_byte_size(dtype):\n",
        "    \"\"\"\n",
        "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
        "    \"\"\"\n",
        "    import re\n",
        "    if dtype == torch.bool:\n",
        "        return 1 / 8\n",
        "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
        "    if bit_search is None:\n",
        "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
        "    bit_size = int(bit_search.groups()[0])\n",
        "    return bit_size // 8\n",
        "\n",
        "def compute_module_sizes(model):\n",
        "    \"\"\"\n",
        "    Compute the size of each submodule of a given model.\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    module_sizes = defaultdict(int)\n",
        "    for name, tensor in named_module_tensors(model, recurse=True):\n",
        "      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
        "      name_parts = name.split(\".\")\n",
        "      for idx in range(len(name_parts) + 1):\n",
        "        module_sizes[\".\".join(name_parts[:idx])] += size\n",
        "\n",
        "    return module_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "m5OoYpcWJlIu",
        "outputId": "d3c83b99-f0f0-40d5-f7d4-1312dc788b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model size is 0.307844608 GB\n"
          ]
        }
      ],
      "source": [
        "module_sizes = compute_module_sizes(model)\n",
        "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q3Q4qsfJlIu"
      },
      "source": [
        "Flan-T5 almost contains 75 million parameters, each parameter is FP32 (as 8-bit=1 byte; 32 bits=4 bytes). 75x$10^6$x4 = 300 million bytes (MB) = 0.3 GB.\n",
        "\n",
        "## Quantizing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "bi137m3IJlIu"
      },
      "outputs": [],
      "source": [
        "from quanto import quantize, freeze"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the model has many layers we will try to quantize only the linear layers. To quantize the model we just need to call **quantize()** method of quanto library. Here we will just quantize the weights into integers not the activations."
      ],
      "metadata": {
        "id": "6DwHiIyqJ1SC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "48uKhxaeJlIu"
      },
      "outputs": [],
      "source": [
        "quantize(model, weights=torch.int8, activations=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "KnXwEyLhJlIu",
        "outputId": "d344e5c8-1b2e-4045-da48-87490baeb879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
            "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OejitJBxJlIu"
      },
      "source": [
        "Here we can see that all the Linear layers are now replaced by QLinear (quantized linear). After this to get the quantized model we just need to call **freeze()** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "-vYv1RrUJlIv"
      },
      "outputs": [],
      "source": [
        "freeze(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "id": "DubJNr8eJlIv",
        "outputId": "8087b75e-ce2d-405f-f4fa-5042136444f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model size is 0.12682868 GB\n"
          ]
        }
      ],
      "source": [
        "module_sizes = compute_module_sizes(model)\n",
        "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPlq1f6KJlIv"
      },
      "source": [
        "Let's check if ther is any performance degredation or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "id": "09gITNq4JlIv",
        "outputId": "f7ae2a9c-0e81-4024-84c2-83fdf042852e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> annie scott</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Hello, my name is \"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory for Linear Quantization\n",
        "formula: **$r = s(q-z)$**, where r is the original value (FP32), s is scale (FP32), q is quantized value (int8), z is zero point (int8).<br>\n",
        "Linear quantization maps the floating point range [$r_{min}$, $r_{max}$] to the quantized range  [$q_{min}$, $q_{max}$].<br>\n",
        "If we look at the extreme values we should get:<br>\n",
        "**$r_{min}$ = s($q_{min}$-z)** & **$r_{maxmax}$ = s($q_{max}$-z)**\n",
        "\n",
        "if we subtract the first equation from the second one we get the scale s:<br>\n",
        "s = ($r_{max}$ - $r_{min}$) / ($q_{max}$ - $q_{min}$)\n",
        "\n",
        "For the zero-point z, we need to round the value since it is a n-bit integer:<br>\n",
        "z = int(round($q_{min}$ - $r_{min}$)/s)\n",
        "\n",
        "## Intermediate State\n",
        "The quanto library creates an intermediate state after we call quantize. Then we call freeze to get the quantized weights. These intemediate are useful for two things:\n",
        "1. When we run the inference on a model by passing an input such as image, a text, etc the activation of the model will vary dpeending on the input to get good linear paramters.\n",
        "\n",
        "## Calibration\n",
        "1. Calibrate model when the activations of the model:\n",
        "- Range of activation values depends on what input was given.\n",
        "- eg. a different input text will generate different activations.\n",
        "2. Min/Max of activation ranges are used to perform linear quantization.\n",
        "3. How to get min and mox arange to activation?\n",
        "- gather sample input data.\n",
        "- run inference\n",
        "- calculate min/max of activations\n",
        "\n",
        "## Qunatization Aware Timing\n",
        "Training in a way that controls how the model performs once it is quantized.\n",
        "1. Intermediate stat holds both:\n",
        "- A quantized version of wights.\n",
        "- Orignial unquantized weights.\n",
        "2. Forward pass (inference)\n",
        "- Use quantized version of model weights to make predictions eg. BF16.\n",
        "3. Back propagation (updating model weights)\n",
        "- Update original, unquantized version of model weights eg. in FP32."
      ],
      "metadata": {
        "id": "_8fv-jNKJ9Ij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "8xixBzsOJlIv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4V_amrl-xG9D",
        "dODA6rR0z297"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}