{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Types and Sizes"
      ],
      "metadata": {
        "id": "Dt5m_y04PePq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "legHVj0-OD9P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integers\n",
        "An unsigned integer data type is used to represent a positive integer. Range of an n-bit unsigned integer **[0, $2^{n-1}$]**. Minimum value of 8-bit integer is 0 and maximum is 255. The computer allocates a sequence of 8 bits to store the 8-bit integer. For an unsigned integer, the decoding process is: if the bit is 0 then value is 0, if the bit is 1 then the decoded value is a power of 2, for first bit the value is $2^0$, for second bit the value is $2^1$, and so on for the 8th bit the value is $2^7$.<br>\n",
        "Example with 8-bit (torch.uint8), the allocated sequence of bits [1,0,0,0,1,0,0,1] → indexing starts from right to left → 20+0+0+23+0+0+0+27=137.\n",
        "For signed integers (used to represent negative or positive integers), **2's complement** representation is used. Range is **[-$2^{n-1}$, $2^{n-1}$-1]**. Example with 8-bit (torch.int8) [-128, 127]. Here the bit in the last position (left most) will have a negative value, for n-bit (-$2^{n-1}$). Example [1,0,0,0,1,0,0,1] → 20+0+0+23+0+0+0+(-27) = 119.\n",
        "\n",
        "## Integers in PyTorch\n",
        "8-bit signed integer → torch.int8<br>\n",
        "8-bit unsigned integer → torch.uint8<br>\n",
        "16-bit signed integer → torch.int16 → torch.short<br>\n",
        "32-bit signed integer → torch.int32 → torch.int<br>\n",
        "64-bit signed integer → torch.int64 → torch.long<br>\n",
        "\n",
        "Here we will be using **torch.iinfo()** method of PyTorch. This function is similar to that of the NumPy function, **np.iinfo()** which returns information about the data types with the smallest and largest values that can be represented by that type."
      ],
      "metadata": {
        "id": "0G_9ZX49OJ-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of 8-bit unsigned integer\n",
        "torch.iinfo(torch.uint8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOX04i8IPPxz",
        "outputId": "72f84a17-a199-42ba-ca71-a6d7fd2f5c0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iinfo(min=0, max=255, dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of 8-bit signed integer\n",
        "torch.iinfo(torch.int8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jucxTlCtQOfX",
        "outputId": "4c8d7d48-fe9f-4ba6-ffa0-91a742a97783"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iinfo(min=-128, max=127, dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of 16-bit signed integer\n",
        "torch.iinfo(torch.int16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROY2d_j-QfSC",
        "outputId": "38bc2b40-c527-4867-d90e-2bba43196a43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iinfo(min=-32768, max=32767, dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of 32-bit signed integer\n",
        "torch.iinfo(torch.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz6U-xd6QiJ6",
        "outputId": "3169dbf9-655d-45bf-ece2-4f395c352673"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of 64-bit signed integer\n",
        "torch.iinfo(torch.int64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr_U8tQxQjd0",
        "outputId": "8fc9ca79-c8e7-4f6d-9ca0-6cfca857f371"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "iinfo(min=-9.22337e+18, max=9.22337e+18, dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Floating Point**\n",
        "3 components:\n",
        "- **sign :-** positive, negative and always 1 bit\n",
        "- **exponent :-** range, impact the representable range of number\n",
        "- **fraction :-** precision, impact on the precision of the number\n",
        "\n",
        "Here precision means defining a number as 0.4999999 or just 0.5.<br>\n",
        "**FP32, BF16, FP16, FP8** are floating point formats with a specific number of bits for exponent and the fraction.<br>\n",
        "\n",
        "**1. Floating Point 32**<br>\n",
        "- **sign :-** 1 bit\n",
        "- **exponent (range) :-** 8 bit\n",
        "- **fraction (precision) :-** 23 bit\n",
        "- **Total :-** 32 bit<br>\n",
        "For positive values we can define very small numbers as **$10^{-45}$** and as big as **$10^{38}$**. For negative values the range is the same with a minus sign in front.<br>\n",
        "For FP we have two formulas to decode the sequence. First to represent very small values which are also called **subnormal values (E=0) -1SF2-126** and second to represent very big values called **normal values (E!=0) -1S(1+F)2E-127**. This data type is very important in ML since most models store weights in FP32.\n",
        "\n",
        "**2. Floating Point 16**<br>\n",
        "- **sign :-** 1 bit\n",
        "- **exponent (range) :-** 5 bit\n",
        "- **fraction (precision) :-** 10 bit\n",
        "- **Total :-** 16 bit<br>\n",
        "Here we have only 6 bits for the exponent and 10 for fraction. So the smallest positive value is **$10^{-8}$** and the biggest is **$10^{4}$**.\n",
        "\n",
        "**2. Brain Floating Point 16**<br>\n",
        "- **sign :-** 1 bit\n",
        "- **exponent (range) :-** 8 bit\n",
        "- **fraction (precision) :-** 7 bit\n",
        "- **Total :-** 16 bit<br>\n",
        "Here we have 8 bits for the exponent and 7 for fraction. So the smallest positive value is **$10^{-41}$** and the biggest is **$10^{38}$**. Compared with FP16 we have more range to store. But the downside is the precision.\n",
        "\n",
        "FP32 → best precision   → max ~**$10^{38}$**<br>\n",
        "FP16 → better precision → max ~**$10^{4}$**<br>\n",
        "BF16 → good precision  → max ~**$10^{38}$**<br>\n",
        "\n",
        "## **FP in PyTorch**\n",
        "16-bit floating point → torch.float16 → torch.half<br>\n",
        "16-bit brain floating point → torch.bfloat16<br>\n",
        "32-bit floating point → torch.float32 → torch.float<br>\n",
        "64-bit floating point → torch.float64 → torch.double"
      ],
      "metadata": {
        "id": "PfvgD2N1Qwfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# by default, python stores float data in FP64\n",
        "value = 1/3"
      ],
      "metadata": {
        "id": "-BlVwrNhQk0P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the number that we stored till 60 decimal values\n",
        "format(value, '.60f')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pXA3pxlnSvzP",
        "outputId": "a56c4e2d-590d-4abf-b26e-5943a0701f5d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.333333333333333314829616256247390992939472198486328125000000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 64-bit floating point\n",
        "tensor_fp64 = torch.tensor(value, dtype = torch.float64)\n",
        "print(f\"FP64 tensor: {format(tensor_fp64.item(), '.60f')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_aTiCgQSzh8",
        "outputId": "ed8df08a-5568-4a87-8759-200726dd242e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
        "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
        "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)"
      ],
      "metadata": {
        "id": "LJDYX3lgS-sO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
        "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
        "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
        "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT-N2Z-YTBxK",
        "outputId": "e3600a97-866e-4176-ea24-ec22994cbeab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n",
            "fp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\n",
            "fp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\n",
            "bf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the **less bits** we have, the **less precise** the approximation will be. As mentioned above precision is worst for bfloat16 we can clearly see that it only gives the value till 9 decimal places."
      ],
      "metadata": {
        "id": "yyOeuQ-dTbSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of `16-bit floating point`\n",
        "torch.finfo(torch.float16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1NvmeH7TB_Y",
        "outputId": "46cc8522-242d-474e-b75a-7400bb9e9504"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of `16-bit brain floating point`\n",
        "torch.finfo(torch.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cllc7rRqTGFU",
        "outputId": "2fdc1830-53f0-4bf3-b6bf-9f59850a6ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of `32-bit floating point`\n",
        "torch.finfo(torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz2OqHdpTGag",
        "outputId": "878f60ba-a8ab-4189-90cf-69963d4a4dd2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of `64-bit floating point`\n",
        "torch.finfo(torch.float64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME9Y1TF-TJhj",
        "outputId": "ea99fe79-4079-4a02-8ec0-9fbedf357479"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "finfo(resolution=1e-15, min=-1.79769e+308, max=1.79769e+308, eps=2.22045e-16, smallest_normal=2.22507e-308, tiny=2.22507e-308, dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Downcasting**\n",
        "Downcasting happens when we convert a higher data type to a lower data type. The value will be converted to the nearest value in the lower data type. For example **FP32 = 0.1** downcasted to an 8-bit integer will be converted to **0**, hence there is **loss of data**.\n",
        "\n",
        "#### **Advantages**\n",
        "1. Reduced memory footprint.\n",
        "- More efficient use of GPU memory.\n",
        "- Enables the training of larger models.\n",
        "- Enables larger batch sizes.\n",
        "2. Increased computation and speed.\n",
        "- Computation using low precision (FP16, BF16) can be faster than FP32 since it requires less memory.\n",
        "- Depending on the hardware (Google TPU, NVIDIA A100).\n",
        "\n",
        "#### **Disadvantages**\n",
        "- Less precision :- we are using less memory, hence computation is less precise.\n",
        "\n",
        "#### **Use case of downcasting**\n",
        "Mixed precision training\n",
        "- Do computation in smaller precision (FP16/BF16/FP8).\n",
        "- Store and update the weights in higher precision (FP32)."
      ],
      "metadata": {
        "id": "VivAgqFxT9Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random pytorch tensor: float32, size=1000\n",
        "tensor_fp32 = torch.rand(1000, dtype = torch.float32)"
      ],
      "metadata": {
        "id": "7AgkquA-TLOD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first 5 elements of the random tensor\n",
        "tensor_fp32[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmIT1klyU_Lf",
        "outputId": "f18646bf-530a-4aa4-c6c3-515165844d8e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9997, 0.9861, 0.8572, 0.2733, 0.2319])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downcast the tensor to bfloat16 using the \"to\" method\n",
        "tensor_fp32_to_bf16 = tensor_fp32.to(dtype = torch.bfloat16)"
      ],
      "metadata": {
        "id": "KFpYQZBKVBCn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_fp32_to_bf16[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nuDMQSYVErV",
        "outputId": "88229cec-8c52-4724-81ec-78c6e20eb28d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 0.9844, 0.8555, 0.2734, 0.2314], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that after downcasting the values are changed but they are very close to the original ones. Let's check the impact of downcasting on multiplication. For this we will use the **.dot()** method of PyTorch."
      ],
      "metadata": {
        "id": "MuLaMow0VYoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor_fp32 x tensor_fp32\n",
        "m_float32 = torch.dot(tensor_fp32, tensor_fp32)"
      ],
      "metadata": {
        "id": "l6HohrJ3VF14"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_float32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjairIEeVHtB",
        "outputId": "4d58f91a-5d7f-4060-d4c7-85109a7092f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(313.5908)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor_fp32_to_bf16 x tensor_fp32_to_bf16\n",
        "m_bfloat16 = torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)"
      ],
      "metadata": {
        "id": "vhCq9pBuVInk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_bfloat16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn8QqytyVJ-d",
        "outputId": "714a085b-0360-44f1-ffa3-aae590dad68f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(314., dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8COwzcYVK-z"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}