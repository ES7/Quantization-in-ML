# Quantization-in-ML
Large generative AI models like LLMs can be so huge that they are hard to run on consumer grade hardware. Quantization has emerged as a key tool for making this possible. How to decide whether we should use int8 or float16 to compress the model? AI models are getting bigger and bigger, so quantization has been recently exciting for the AI community because it enables us to shrink models to a small size, so that anyone can run it with their own computer with little to no performance degradation.

**1. `Data_Types_&_Sizes.ipynb`:** In this notebook I have explained the significance of different data types and how to convert parameters from one data type to another.

**2. `Loading_Models_in_different_dtypes.ipynb`:** In this notebook I have explained how we can load models in different data types to save memory and what are its effects on the model's performace.

**3. `Quantization_Theory.ipynb`:** In this notebook I have explained how to apply linear quantization to any model and the maths behind it.

