{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a52b6b-9aae-46d8-975a-9dce4a3b0fd9",
   "metadata": {},
   "source": [
    "# Custom Build an 8-Bit Quantizer\n",
    "Here we will create a quantizer which can quantize any model in 8-bit precision using per channel quantization scheme. The quantizer is modality agnostic meaning we can apply it on vision, audio, text, and even multimodal models.<br>\n",
    "- **Step 1 :-** creating a **`W8A16LinearLayer`** class to store 8-bit weights and scales\n",
    "- **Step 2 :-** replacing all **`torch.nn.Linear`** layers with **`W8A16LinearLayer`**\n",
    "- **Step 3 :-** building a quantizer and quantize a model end-to-end\n",
    "- **Step 4 :-** test the naive absmax quantization on many scenario and study its impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afeb7dce-ff99-478b-a9e5-273e9779075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7d70f2-cb89-400a-b829-460b8d85a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf6829-2ba0-46d9-a152-df7848fc12f6",
   "metadata": {},
   "source": [
    "**NOTE:** weight matrix has the shape **(output dimension, input dimension)**. When we perform the matrix mulitplication between the int8 matrix and the hidden states, we will have a vector of batch size output dimension. So it is important that the scales have the same shape as the output shape of the weight matrix and same for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dae42d3-38a9-413b-85ff-4ecf51dc228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-125.0000,  240.0000, -346.0000, -524.0000, -436.0000, -185.0000,\n",
       "         -150.0000,  -17.6250,  160.0000, -330.0000, -330.0000,  328.0000,\n",
       "         -225.0000,  418.0000,  580.0000, -136.0000, -122.0000,   31.1250,\n",
       "           32.2500, -107.5000,  169.0000,   36.0000,  276.0000,  -33.5000,\n",
       "         -380.0000,  143.0000,   97.0000, -162.0000, -199.0000,   74.0000,\n",
       "          159.0000, -612.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda6e4f-8580-4a1d-b864-cc6c3792a071",
   "metadata": {},
   "source": [
    "First we have cast the weights into the same data type as the hidden states. Then on top of this we will perform matrix multiplication via **`F.linear()`** function from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221e89b5-aeb0-454c-b43c-6207ffb417da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -3.7188,  284.0000, -264.0000, -752.0000, -154.0000, -158.0000,\n",
       "         -131.0000,  -14.3125,  100.5000, -430.0000,  109.0000, -416.0000,\n",
       "          -32.7500, -103.5000,   24.7500,  -81.5000, -178.0000,   39.5000,\n",
       "           -9.0000,  -28.5000,  103.0000,    4.5625,  -54.7500,    2.4844,\n",
       "          124.0000,  -40.5000,   97.0000,  220.0000,   11.6875,  132.0000,\n",
       "            2.5000,  520.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4787c07-3f9b-481b-85c8-0506e0220757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -3.2031,  286.0000, -266.0000, -752.0000, -155.0000, -158.0000,\n",
       "         -132.0000,  -13.8750,  100.0000, -430.0000,  107.5000, -416.0000,\n",
       "          -33.0000, -104.0000,   26.2500,  -80.5000, -178.0000,   38.7500,\n",
       "           -9.4375,  -28.3750,  104.5000,    5.0312,  -55.7500,    0.8750,\n",
       "          124.0000,  -40.2500,   97.0000,  220.0000,   12.6250,  133.0000,\n",
       "            3.2188,  520.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a21d7-cb8b-4852-a066-c1ba2aec542b",
   "metadata": {},
   "source": [
    "Then we will multiply this with the input scales and optionally add a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f112ef87-add8-488a-acf2-cdef911ec36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5401298d-0f8f-4ef4-a78c-168b6d1406fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bias:\n",
      "\n",
      " tensor([[  -3.2031,  286.0000, -266.0000, -752.0000, -155.0000, -158.0000,\n",
      "         -132.0000,  -13.8750,  100.0000, -430.0000,  107.5000, -416.0000,\n",
      "          -33.0000, -104.0000,   26.2500,  -80.5000, -178.0000,   38.7500,\n",
      "           -9.4375,  -28.3750,  104.5000,    5.0312,  -55.7500,    0.8750,\n",
      "          124.0000,  -40.2500,   97.0000,  220.0000,   12.6250,  133.0000,\n",
      "            3.2188,  520.0000]], dtype=torch.bfloat16)\n",
      "\n",
      "Without bias:\n",
      "\n",
      " tensor([[  -3.7188,  284.0000, -264.0000, -752.0000, -154.0000, -158.0000,\n",
      "         -131.0000,  -14.3125,  100.5000, -430.0000,  109.0000, -416.0000,\n",
      "          -32.7500, -103.5000,   24.7500,  -81.5000, -178.0000,   39.5000,\n",
      "           -9.0000,  -28.5000,  103.0000,    4.5625,  -54.7500,    2.4844,\n",
      "          124.0000,  -40.5000,   97.0000,  220.0000,   11.6875,  132.0000,\n",
      "            2.5000,  520.0000]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142e1d3d-19aa-4ad1-b0be-45ab4aaf189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### running this will result in an error\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    \n",
    "    W8A16LinearLayer(1, 1)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a574edf-93dc-45ee-8f23-3fc7132f9251",
   "metadata": {},
   "source": [
    "When we create an **`nn.parameter`** layer, PyTorch expects that parameter where it is able to compute gradients on it. We can't explicitly compute gradients on **int8 tensors** yet. So we should get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb93aa77-d094-4eab-90a3-f64f350cc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9913-3934-40e8-846d-baeeab069fb9",
   "metadata": {},
   "source": [
    "This is the right approach to store int8 weights is instead of saving attributes as being an endless parameter, is to call a method **`register_buffer()`**. This way instead of storing a parameter, we just store a buffer means we don't need to compute gradients on the tensor, and we can initialize it with whatever dtype we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82083e11-1016-4459-a90e-a459efa7b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instance = W8A16LinearLayer(16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a308687-42cb-421b-9026-c29d25cc045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(dummy_instance.int8_weights.shape)\n",
    "print(dummy_instance.scales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077fe07-c82a-4ed1-9204-eb6d0b8237b0",
   "metadata": {},
   "source": [
    "Creating a forward pass for the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19da72a-9951-4900-b599-16365504daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\"int8_weights\",torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6740f650-5543-4956-8d7d-9a2dca93e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(16, 32)\n",
    "dummy_hidden_states = torch.randn(1, 6, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f490a09-0f95-4c8a-920b-4d216a34f709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3a7332-4436-41b6-a3b5-46b28822f3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a60ae-2e73-474d-ae99-7c0a71cc0f37",
   "metadata": {},
   "source": [
    "We have a linear layer which is working fine and a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "915c2a9b-9d7e-49a8-a03e-bec65f55f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights/scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98ac09-1bc3-44d0-8c2f-bfd7a9357f0a",
   "metadata": {},
   "source": [
    "Here we will add the quantization method. So first upcast the weights into FP32 then find the scale value and make sure that scale has same dtype as input weights. Then using the formula find the int8 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac1777ec-abec-40c8-a8a9-5042c2f7bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef97f9cb-68f2-4fc9-a022-e604aac9baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before:\n",
      " tensor([[  70, -105,   74,   95],\n",
      "        [  48,  -17,  -91,  -97],\n",
      "        [  10,  106,  -92, -108],\n",
      "        [ -44,   70,   -2,  -21],\n",
      "        [  78,   10,  -43,   31],\n",
      "        [ -33,  -82, -113,  -31],\n",
      "        [ -52,   56,  -33,   78],\n",
      "        [ -15,  -35,  -94,  120]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights before:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d552c61-a5cd-4595-b72f-ca9173ed6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf7c633-219c-4f51-ba74-54192b70751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.quantize(random_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18313281-83b7-4f75-ac50-42aab803234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights After:\n",
      " tensor([[  91,   63,  127,   85,   92,  -28,  -98,  -19],\n",
      "        [ -18,   82,  -55,   -2,   30, -128,   40,  121],\n",
      "        [   3, -110,   37,    0,   29,  -70,  127,  -73],\n",
      "        [  76,  -60,   22,  127, -104,  100,   32,  -21]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights After:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7461ded-d2eb-4210-bfdd-aa2d3475e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0086, 0.0110, 0.0102, 0.0144], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db239ebf-5186-4786-8b81-7c9317ff8ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c2dadcc-6b97-4caa-9973-ca7504f0e211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1751ade0-1229-4d57-8a9d-e18adc883aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7812,  0.5430,  1.0938,  0.7305,  0.7930, -0.2412, -0.8438, -0.1631],\n",
       "        [-0.1973,  0.9023, -0.6055, -0.0220,  0.3301, -1.4062,  0.4395,  1.3281],\n",
       "        [ 0.0305, -1.1250,  0.3770,  0.0000,  0.2949, -0.7148,  1.2969, -0.7422],\n",
       "        [ 1.0938, -0.8633,  0.3164,  1.8281, -1.5000,  1.4375,  0.4609, -0.3027]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### dequantized weights\n",
    "module.int8_weights * module.scales.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d91637e-1448-41e5-91b9-7e4ae53493ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8125e-01,  5.4297e-01,  1.0938e+00,  7.3047e-01,  7.8906e-01,\n",
       "         -2.4316e-01, -8.3984e-01, -1.6602e-01],\n",
       "        [-2.0020e-01,  9.0625e-01, -6.0156e-01, -2.4292e-02,  3.2617e-01,\n",
       "          1.3984e+00,  4.4336e-01,  1.3281e+00],\n",
       "        [ 2.6001e-02, -1.1250e+00,  3.7891e-01,  8.8501e-04,  2.9492e-01,\n",
       "         -7.1484e-01,  1.2969e+00, -7.4609e-01],\n",
       "        [ 1.0859e+00, -8.6719e-01,  3.1055e-01,  1.8281e+00, -1.5000e+00,\n",
       "          1.4375e+00,  4.6484e-01, -3.0469e-01]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### original weights\n",
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e3b1ef3-115f-4089-8fdc-e9cae4aa5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0898, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_matrix - module.int8_weights * module.scales.unsqueeze(1)).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c7f68-34ba-4787-90af-46e9422decb5",
   "metadata": {},
   "source": [
    "# Quantization Pipeline\n",
    "Replace all of the `torch.nn.Linear` layers with the `W8A16LinearLayer` layer. Call `quantize` on the linear layers using the original weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef410793-9982-4d68-94df-0095616ed970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "          any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "\n",
    "            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8a442-6cca-437a-9f5c-ec4abc6f6384",
   "metadata": {},
   "source": [
    "We can pass the model also module, target class of the new class that we are going to set in replacement to the linear layer and module name to exclude which is name of the module that we are going to exclude in this replacement logic. For better results it is better to keep the last module unquantized.<br>\n",
    "We are going to simple loop over the modules named **children**, and if the sub module is an instance of an **nn.Linear** and we don't have any name that matches the names that are inside the module name to exclude, then we are going to move forward with the module replacement. So we will get the bias of the sub module in **`old_bias`** because we are going to use it to create the new target class.<br>\n",
    "Then we can create the new module which is target class, the in_features and out_features should be the same as the linear layers and use the same dtype as sub modules weights.<br>\n",
    "Then we will call set attributes function, we will replace the current attribute of module that has name as `'name'` with the `new_module`.<br>\n",
    "And if the old module has a bias then we will explicitly set the bias of the new module to `old_bias`.<br>\n",
    "Then we will recursively call this method on child module by passing the same arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "838aa54c-9499-40f5-81c0-77c4994a790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(1, 1)\n",
    "    # Try with bias\n",
    "    self.linear_1 = nn.Linear(1, 1)\n",
    "    # Try without bias\n",
    "    self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "    # Lm prediction head\n",
    "    self.lm_head = nn.Linear(1, 1, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3c69a-c8cc-4f34-a00b-770de68f7225",
   "metadata": {},
   "source": [
    "For testing purpose we have created a dummy model with two linear layers and one language model head, which is usually the last module in a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49c00741-f36d-4099-b4af-82bff159184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = DummyModel()\n",
    "model_2 = DummyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87f509-1c44-4d65-a6bf-6a210809bf5c",
   "metadata": {},
   "source": [
    "The function changes the layers of model, so we have created two copies one for testing out the model name to exclude feature and another which will replace all linear layer instances with new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "981803da-2406-43b7-8290-4aed0e173059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76849fd-5095-42a9-9c58-b43956dccea2",
   "metadata": {},
   "source": [
    "In the arguments we have specified we don't want to replace the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "941ed017-b6ca-4c58-8134-82d7cd8cc851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target(model_2, W8A16LinearLayer, [])\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9bb1f-e518-476f-8c86-84b656728002",
   "metadata": {},
   "source": [
    "We have passed an empty list, due to which the function will replace all the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31240-8d67-47d5-9d6c-207fad427562",
   "metadata": {},
   "source": [
    "### Linear Layer Replacement + Quantization\n",
    "- Modify the `replace_linear_with_target` function to also perform quantization.\n",
    "- Implement `replace_linear_with_target_and_quantize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6e78e3b-5761-4ad9-9a77-d5aa9ee0968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb66739-c566-4f4d-aa43-9847b9944751",
   "metadata": {},
   "source": [
    "In the same function we will add a line after setting attributes, we will get the attributes and then quantize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bd46c1c-ee25-4688-9a9d-02848427b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "199973fa-a3cf-462a-ad50-203390d62a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize(model_3, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c037c-994e-445b-9c8d-81c2e7ef22e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
