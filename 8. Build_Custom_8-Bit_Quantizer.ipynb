{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a52b6b-9aae-46d8-975a-9dce4a3b0fd9",
   "metadata": {},
   "source": [
    "# Custom Build an 8-Bit Quantizer\n",
    "Here we will create a quantizer which can quantize any model in 8-bit precision using per channel quantization scheme. The quantizer is modality agnostic meaning we can apply it on vision, audio, text, and even multimodal models.<br>\n",
    "- **Step 1 :-** creating a **`W8A16LinearLayer`** class to store 8-bit weights and scales\n",
    "- **Step 2 :-** replacing all **`torch.nn.Linear`** layers with **`W8A16LinearLayer`**\n",
    "- **Step 3 :-** building a quantizer and quantize a model end-to-end\n",
    "- **Step 4 :-** test the naive absmax quantization on many scenario and study its impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afeb7dce-ff99-478b-a9e5-273e9779075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7d70f2-cb89-400a-b829-460b8d85a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf6829-2ba0-46d9-a152-df7848fc12f6",
   "metadata": {},
   "source": [
    "**NOTE:** weight matrix has the shape **(output dimension, input dimension)**. When we perform the matrix mulitplication between the int8 matrix and the hidden states, we will have a vector of batch size output dimension. So it is important that the scales have the same shape as the output shape of the weight matrix and same for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dae42d3-38a9-413b-85ff-4ecf51dc228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-148.0000,  -56.2500,  192.0000,   74.5000,  103.0000, -189.0000,\n",
       "          324.0000,  -29.8750,  250.0000,   33.5000,  -50.2500,  -67.5000,\n",
       "          -69.5000,  316.0000,  -37.2500,   16.0000,  100.0000, -262.0000,\n",
       "          632.0000,  -53.2500,  428.0000,  388.0000,  -31.2500,   -6.2188,\n",
       "          -98.5000, -382.0000, -232.0000, -296.0000, -332.0000, -164.0000,\n",
       "           20.0000,  164.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda6e4f-8580-4a1d-b864-cc6c3792a071",
   "metadata": {},
   "source": [
    "First we have cast the weights into the same data type as the hidden states. Then on top of this we will perform matrix multiplication via **`F.linear()`** function from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221e89b5-aeb0-454c-b43c-6207ffb417da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3875e+01, -5.3750e+01,  5.5600e+02, -5.7250e+01, -1.1500e+02,\n",
       "          4.7000e+02, -2.9000e+02,  3.1750e+01, -9.5000e+01,  2.0250e+01,\n",
       "         -3.5625e+00,  6.0500e+01, -7.5195e-02, -5.8800e+02,  7.2500e+01,\n",
       "         -3.1375e+01, -1.9500e+02,  3.2000e+01, -9.4000e+02,  1.1406e+00,\n",
       "          3.9000e+02,  4.1000e+02, -7.8750e+00,  5.1562e+00,  3.3000e+01,\n",
       "          4.9250e+01,  7.0500e+01, -2.7600e+02,  1.0900e+02,  1.9600e+02,\n",
       "          3.3500e+01, -6.0500e+01]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4787c07-3f9b-481b-85c8-0506e0220757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3000e+01, -5.4500e+01,  5.5600e+02, -5.7250e+01, -1.1600e+02,\n",
       "          4.7000e+02, -2.9000e+02,  3.1000e+01, -9.6000e+01,  2.0250e+01,\n",
       "         -2.5938e+00,  5.9500e+01,  2.4512e-01, -5.8800e+02,  7.3500e+01,\n",
       "         -3.1125e+01, -1.9400e+02,  3.2000e+01, -9.4000e+02,  1.1953e+00,\n",
       "          3.9000e+02,  4.1000e+02, -6.7812e+00,  6.0625e+00,  3.3250e+01,\n",
       "          5.0500e+01,  6.9000e+01, -2.7800e+02,  1.0850e+02,  1.9600e+02,\n",
       "          3.4500e+01, -6.1250e+01]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a21d7-cb8b-4852-a066-c1ba2aec542b",
   "metadata": {},
   "source": [
    "Then we will multiply this with the input scales and optionally add a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f112ef87-add8-488a-acf2-cdef911ec36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5401298d-0f8f-4ef4-a78c-168b6d1406fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bias:\n",
      "\n",
      " tensor([[ 2.3000e+01, -5.4500e+01,  5.5600e+02, -5.7250e+01, -1.1600e+02,\n",
      "          4.7000e+02, -2.9000e+02,  3.1000e+01, -9.6000e+01,  2.0250e+01,\n",
      "         -2.5938e+00,  5.9500e+01,  2.4512e-01, -5.8800e+02,  7.3500e+01,\n",
      "         -3.1125e+01, -1.9400e+02,  3.2000e+01, -9.4000e+02,  1.1953e+00,\n",
      "          3.9000e+02,  4.1000e+02, -6.7812e+00,  6.0625e+00,  3.3250e+01,\n",
      "          5.0500e+01,  6.9000e+01, -2.7800e+02,  1.0850e+02,  1.9600e+02,\n",
      "          3.4500e+01, -6.1250e+01]], dtype=torch.bfloat16)\n",
      "\n",
      "Without bias:\n",
      "\n",
      " tensor([[ 2.3875e+01, -5.3750e+01,  5.5600e+02, -5.7250e+01, -1.1500e+02,\n",
      "          4.7000e+02, -2.9000e+02,  3.1750e+01, -9.5000e+01,  2.0250e+01,\n",
      "         -3.5625e+00,  6.0500e+01, -7.5195e-02, -5.8800e+02,  7.2500e+01,\n",
      "         -3.1375e+01, -1.9500e+02,  3.2000e+01, -9.4000e+02,  1.1406e+00,\n",
      "          3.9000e+02,  4.1000e+02, -7.8750e+00,  5.1562e+00,  3.3000e+01,\n",
      "          4.9250e+01,  7.0500e+01, -2.7600e+02,  1.0900e+02,  1.9600e+02,\n",
      "          3.3500e+01, -6.0500e+01]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142e1d3d-19aa-4ad1-b0be-45ab4aaf189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### running this will result in an error\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    \n",
    "    W8A16LinearLayer(1, 1)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a574edf-93dc-45ee-8f23-3fc7132f9251",
   "metadata": {},
   "source": [
    "When we create an **`nn.parameter`** layer, PyTorch expects that parameter where it is able to compute gradients on it. We can't explicitly compute gradients on **int8 tensors** yet. So we should get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb93aa77-d094-4eab-90a3-f64f350cc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9913-3934-40e8-846d-baeeab069fb9",
   "metadata": {},
   "source": [
    "This is the right approach to store int8 weights is instead of saving attributes as being an endless parameter, is to call a method **`register_buffer()`**. This way instead of storing a parameter, we just store a buffer means we don't need to compute gradients on the tensor, and we can initialize it with whatever dtype we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82083e11-1016-4459-a90e-a459efa7b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instance = W8A16LinearLayer(16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a308687-42cb-421b-9026-c29d25cc045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(dummy_instance.int8_weights.shape)\n",
    "print(dummy_instance.scales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077fe07-c82a-4ed1-9204-eb6d0b8237b0",
   "metadata": {},
   "source": [
    "Creating a forward pass for the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19da72a-9951-4900-b599-16365504daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\"int8_weights\",torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6740f650-5543-4956-8d7d-9a2dca93e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(16, 32)\n",
    "dummy_hidden_states = torch.randn(1, 6, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f490a09-0f95-4c8a-920b-4d216a34f709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3a7332-4436-41b6-a3b5-46b28822f3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a60ae-2e73-474d-ae99-7c0a71cc0f37",
   "metadata": {},
   "source": [
    "We have a linear layer which is working fine and a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "915c2a9b-9d7e-49a8-a03e-bec65f55f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\"int8_weights\", torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scales\", torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.randn((1, out_features), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights/scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98ac09-1bc3-44d0-8c2f-bfd7a9357f0a",
   "metadata": {},
   "source": [
    "Here we will add the quantization method. So first upcast the weights into FP32 then find the scale value and make sure that scale has same dtype as input weights. Then using the formula find the int8 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac1777ec-abec-40c8-a8a9-5042c2f7bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef97f9cb-68f2-4fc9-a022-e604aac9baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before:\n",
      " tensor([[  24,  104,   -4,   70],\n",
      "        [   1,  116,  -41,   90],\n",
      "        [  85, -125,   43,   49],\n",
      "        [ -60,   25, -125,   85],\n",
      "        [  22, -103,  125, -106],\n",
      "        [-103,  -39,   84, -124],\n",
      "        [  45,  -84,   55,   -1],\n",
      "        [ -80,  -81,  -53,  -11]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights before:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d552c61-a5cd-4595-b72f-ca9173ed6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf7c633-219c-4f51-ba74-54192b70751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.quantize(random_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18313281-83b7-4f75-ac50-42aab803234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights After:\n",
      " tensor([[ -16,  -23,  127,    1,   26,   11,  -28,    9],\n",
      "        [  95,  -14,   24,  -23,    4, -105, -127,  -43],\n",
      "        [  41,  -49,  -88,   46, -128,   72,  -34,  104],\n",
      "        [ -34,  -53,  -53,   66, -128,   19,    1,  -82]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights After:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7461ded-d2eb-4210-bfdd-aa2d3475e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0187, 0.0146, 0.0107, 0.0114], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db239ebf-5186-4786-8b81-7c9317ff8ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c2dadcc-6b97-4caa-9973-ca7504f0e211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1751ade0-1229-4d57-8a9d-e18adc883aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2988, -0.4297,  2.3750,  0.0187,  0.4863,  0.2051, -0.5234,  0.1680],\n",
       "        [ 1.3906, -0.2051,  0.3516, -0.3359,  0.0586, -1.5391, -1.8594, -0.6289],\n",
       "        [ 0.4414, -0.5273, -0.9453,  0.4941, -1.3750,  0.7734, -0.3652,  1.1172],\n",
       "        [-0.3867, -0.6016, -0.6016,  0.7500, -1.4531,  0.2158,  0.0114, -0.9297]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### dequantized weights\n",
    "module.int8_weights * module.scales.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d91637e-1448-41e5-91b9-7e4ae53493ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3086, -0.4355,  2.3750,  0.0231,  0.4941,  0.2031, -0.5273,  0.1641],\n",
       "        [ 1.3906, -0.2031,  0.3516, -0.3340,  0.0608, -1.5391, -1.8594, -0.6250],\n",
       "        [ 0.4414, -0.5234, -0.9531,  0.4961, -1.3672,  0.7773, -0.3594,  1.1250],\n",
       "        [-0.3887, -0.6055, -0.5977,  0.7461, -1.4453,  0.2139,  0.0116, -0.9258]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### original weights\n",
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e3b1ef3-115f-4089-8fdc-e9cae4aa5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0036, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_matrix - module.int8_weights * module.scales.unsqueeze(1)).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b223d18-0150-4647-8122-0fd1677c9ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
