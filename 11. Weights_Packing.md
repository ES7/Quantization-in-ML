# Weights Packing
Assume we want to quantize our model in 4-bit precision, and we want to store the weights in a
torch tensor. So ideally we want to create a tensor with some values and pass `dtype=torch.int4`.
But the problem is there is no native support for 4-bit weights in PyTorch.
```python
import torch
tensor = torch.tensor([0,1], dtype=torch.int4) # is not supported!
```
The only possible solution is instead of saving the tensor in 4-bit we have to save it in 8-bit
as currently it's the dtype with smallest precision that is available in PyTorch. So in practice
we need to save the tensor in 8-bit precision. But this will overhead for large models therefore if we go for native approach, means if we store the 4-bit weights in an 8-bit tensor, 
there will be no point quantizing the model into 4-bit because all the parameters will be stored in 8-bit precision. So for that we need to pack the 4-bit weights into 8-bit tensor.

## How does packing works?
Consider the tensor given bbelow that stores 4 values that can be represented in 2-bit precision, but stored in 8-bit.
```python
import torch
unpacked_tensor = torch.tensor([1, 0, 3, 2], dtype=torch.int8)
```
Currently this tensor is stored as [00000001  00000000  00000011  00000010]. This is not really optimal as we need to allocate 4 times 8-bit terms of memory in order to store weights that can be encoded only in 2-bit.<br>
To solve this we can "pack" all these data points into a single 8-bit tensor as [10110001]-->(10,11,00,01). This value in unit8 will end up being 177.
```python
packed_tensor = torch.tensor([177], dtype=torch.int8)
```
#### Advantages:
- It reflects the "true" memory footprint of the quantized weights
#### Disadvantages:
- The unpacked tensors need to be a shape with a multiple of 8//nbits
- It needs to unpack before performing an operation


