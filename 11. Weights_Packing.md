# Weights Packing
Assume we want to quantize our model in 4-bit precision, and we want to store the weights in a
torch tensor. So ideally we want to create a tensor with some values and pass `dtype=torch.int4`.
But the problem is there is no native support for 4-bit weights in PyTorch.
```python
import torch
tensor = torch.tensor([0,1], dtype=torch.int4) # is not supported!
```
The only possible solution is instead of saving the tensor in 4-bit we have to save it in 8-bit
as currently it's the dtype with smallest precision that is available in PyTorch. So in practice
we need to save the tensor in 8-bit precision. 
